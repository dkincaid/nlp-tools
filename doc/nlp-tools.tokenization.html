<!DOCTYPE html>
<html><head><link href="css/default.css" rel="stylesheet" type="text/css"><script src="js/jquery.min.js" type="text/javascript"></script><script src="js/page_effects.js" type="text/javascript"></script><title>nlp-tools.tokenization documentation</title></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html">Nlp-tools 0.0.1-SNAPSHOT API documentation</a></h1></div><div class="sidebar" id="namespaces"><h3><span>Namespaces</span></h3><ul><li><a href="nlp-tools.corenlp.html"><span>nlp-tools.corenlp</span></a></li><li><a href="nlp-tools.similarity.html"><span>nlp-tools.similarity</span></a></li><li class="current"><a href="nlp-tools.tokenization.html"><span>nlp-tools.tokenization</span></a></li></ul></div><div class="sidebar" id="vars"><h3>Public Vars</h3><ul><li><a href="nlp-tools.tokenization.html#var-lucene-tokenizer"><span>lucene-tokenizer</span></a></li><li><a href="nlp-tools.tokenization.html#var-simple-tokenizer"><span>simple-tokenizer</span></a></li><li><a href="nlp-tools.tokenization.html#var-standard-tokenizer"><span>standard-tokenizer</span></a></li><li><a href="nlp-tools.tokenization.html#var-whitespace-tokenizer"><span>whitespace-tokenizer</span></a></li></ul></div><div class="namespace-docs" id="content"><h2>nlp-tools.tokenization documentation</h2><pre class="doc"></pre><div class="public" id="var-lucene-tokenizer"><h3>lucene-tokenizer</h3><div class="usage"><code>(lucene-tokenizer analyzer text)</code></div><pre class="doc">Uses the supplied Lucene Analyzer to tokenize the given text. Returns a vector containing the tokens.
</pre><div class="src-link"><a href="http://github.com/dkincaid/nlp-tools/blob/develop/src/nlp_tools/tokenization.clj">Source</a></div></div><div class="public" id="var-simple-tokenizer"><h3>simple-tokenizer</h3><div class="usage"><code>(simple-tokenizer text)</code></div><pre class="doc">Uses the OpenNLP SimpleTokenizer which tokenizes on character class to tokenize the given text.
Returns a vector containing the tokens.</pre><div class="src-link"><a href="http://github.com/dkincaid/nlp-tools/blob/develop/src/nlp_tools/tokenization.clj">Source</a></div></div><div class="public" id="var-standard-tokenizer"><h3>standard-tokenizer</h3><div class="usage"><code>(standard-tokenizer text)</code></div><pre class="doc">Uses the Lucene StandardTokenizer to tokenize the given text. Returns a vector containing
the tokens.</pre><div class="src-link"><a href="http://github.com/dkincaid/nlp-tools/blob/develop/src/nlp_tools/tokenization.clj">Source</a></div></div><div class="public" id="var-whitespace-tokenizer"><h3>whitespace-tokenizer</h3><div class="usage"><code>(whitespace-tokenizer text)</code></div><pre class="doc">Uses the Lucene WhitespaceTokenizer to tokenize the given text. Returns a vector containing
the tokens.</pre><div class="src-link"><a href="http://github.com/dkincaid/nlp-tools/blob/develop/src/nlp_tools/tokenization.clj">Source</a></div></div></div></body></html>